{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k1p8OJ57gmvg"
   },
   "source": [
    "# Setup and Preparation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oJz1K95L3nW7",
    "outputId": "10082bbe-958f-4a8d-f23d-3dad31441aa3"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CbbHPl2GFgv0",
    "outputId": "37019c15-8768-4b6d-c5e9-34bb953712f1"
   },
   "outputs": [],
   "source": [
    "%cd drive/MyDrive/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3RC9UZXyFp5i"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pPEb9kyrFt-2"
   },
   "outputs": [],
   "source": [
    "# data cleaning and prepration and test split\n",
    "df_android = pd.read_csv('AndroidFields.csv')\n",
    "mask_new = df_android['Status'] == 'New'\n",
    "df_android = df_android[~mask_new]\n",
    "df_android.reset_index(inplace = True)\n",
    "# From Frank: we do need some of the features so don't drop them. For example, BugID and MergeID are used to locate duplicates and their original bugs.\n",
    "df_android = df_android.drop(['crypto', 'general', 'java', 'networking', 'MasterID','Product','Component','Priority','PriorityNumber','Version','OpenDate','CloseDate','Stars','VersionNumber','Summary','index'],axis=1)\n",
    "df_android['Label'] = df_android['Status']\n",
    "\n",
    "df_eclipse = pd.read_csv('EclipseFields.csv')\n",
    "df_eclipse.reset_index(inplace = True)\n",
    "df_eclipse = df_eclipse.drop(['crypto', 'general', 'java', 'networking', 'MasterID','Priority','PriorityNumber','Version','OpenDate','CloseDate','Stars','VersionNumber','Summary','index'],axis=1)\n",
    "df_eclipse['Label'] = df_eclipse['Status']\n",
    "\n",
    "android_train, android_test = train_test_split(df_android, test_size=0.3, random_state=42)\n",
    "eclipse_train, eclipse_test = train_test_split(df_eclipse, test_size=0.3, random_state=42) # From Frank: Added this for eclipse dataset\n",
    "\n",
    "# android_train.dropna()\n",
    "# # android_test.dropna()\n",
    "# eclipse_train.dropna()\n",
    "# eclipse_test.dropna()\n",
    "# train.to_csv(\"bug_train.csv\")\n",
    "# test.to_csv(\"bug_test.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7kWfhi-T__L"
   },
   "source": [
    "# Preliminary Preprocessing\n",
    "\n",
    "Including tasks like lowercasing, punctuation removal, stop word removal, repetitive word removal, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ws4CBSXZh33o"
   },
   "source": [
    "Temporary block for some preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EMbxoIiNSCo8",
    "outputId": "61073e49-d02b-48e5-b4ef-63a157d45694"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Temporary preproccessing\n",
    "# From Frank: Personally I think we should keep this. Bert Tokenizer doesn't really\n",
    "#             do these things. It looks like BERT tokenizer will process the text\n",
    "#             for it to only be better utilized by BERT. So they are doing different jobs.\n",
    "def preprocessing_text(text):\n",
    "  # Tokenization\n",
    "  tokens = word_tokenize(text)\n",
    "\n",
    "  # Lowercasing and punctuation removal\n",
    "  tokens = [word.lower() for word in tokens if word.isalpha()]\n",
    "\n",
    "  # Stop word removal\n",
    "  stop_words = set(stopwords.words('english'))\n",
    "  tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "  # Stemming (Word root extraction)\n",
    "  stemmer = PorterStemmer()\n",
    "  tokens = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "  # Remove repetitive words (not included in the original paper)\n",
    "  tokens = list(set(tokens))\n",
    "\n",
    "  return ' '.join(tokens) # put words back into a sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8EeasEMqMi9F"
   },
   "source": [
    "# Topic Modelling and Feature Selection\n",
    "\n",
    "Perform Topic Modelling for all bug reports. First categorize all reports based on their status/resolutions, and then categorize them into 10 topics, and then do feature selection inside each topic separately.\n",
    "\n",
    "At the end of this block, there are two dataframes that can be used for training/evaluating.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4hKoFu43nahz"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import chi2\n",
    "from scipy.stats import pearsonr, chi2_contingency\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Topic modelling should be performed on reports that have the same status/resolution.\n",
    "# This method will categorize all reports based on their status/resolution.\n",
    "# It returns a list of dfs, each of which has the same status/resolution.\n",
    "def categorize_reports(df, status_list):\n",
    "  categories = []\n",
    "  for status in status_list:\n",
    "    filtered_reports = df[(df['Status'].str.lower() == status) | ((df['Status'].str.lower() == 'duplicate') & (df['MergeID'].isin(df[df['Status'].str.lower() == status]['BugID'])))]\n",
    "    new_df = pd.DataFrame(filtered_reports)\n",
    "    categories.append(new_df)\n",
    "  #add orphan reports\n",
    "  filtered_reports = df[\n",
    "    (df['Status'].str.lower() == 'duplicate') &\n",
    "    ~(df['MergeID'].isin(df[df['Status'].str.lower() == status]['BugID']))\n",
    "  ]\n",
    "  new_df = pd.DataFrame(filtered_reports)\n",
    "  categories.append(new_df)\n",
    "  return categories\n",
    "\n",
    "# Perform topic modelling for one category\n",
    "# It returns ten dfs in a list in which the reports share the same topic\n",
    "def topic_modelling(category_df):\n",
    "  documents = category_df['all_textual_data'].tolist()\n",
    "  # Convert text data to document-term matrix using CountVectorizer\n",
    "  vectorizer = CountVectorizer()\n",
    "  X = vectorizer.fit_transform(documents)\n",
    "  # Create a gensim Dictionary\n",
    "  gensim_dict = Dictionary([document.split() for document in documents])\n",
    "  # Convert document-term matrix to gensim corpus\n",
    "  corpus = [gensim_dict.doc2bow(document.split()) for document in documents]\n",
    "  # The number of topics\n",
    "  num_topics = 10\n",
    "\n",
    "  # Perform topic modeling using LDA from gensim\n",
    "  lda_model = LdaModel(corpus=corpus, id2word=gensim_dict, num_topics=num_topics, passes=1) # change passes for faster/better convergence\n",
    "\n",
    "  # Display the topics and associated words\n",
    "  # for idx, topic in lda_model.print_topics(-1):\n",
    "  #     print(f\"Topic {idx}: {topic}\")\n",
    "\n",
    "  # Get the topics for each document\n",
    "  document_topics = []\n",
    "  for i, doc in enumerate(corpus):\n",
    "    topics = lda_model.get_document_topics(doc)\n",
    "    dominant_topic = max(topics, key=lambda x: x[1])\n",
    "    document_topics.append((i, dominant_topic[0]))\n",
    "\n",
    "  # Create separate DataFrames for each topic\n",
    "  topic_list = []\n",
    "  for i in range(num_topics):\n",
    "      report_index_list = [doc_id for doc_id, topic_id in document_topics if topic_id == i]\n",
    "      topic_df = category_df.iloc[report_index_list].copy()\n",
    "      topic_list.append(topic_df)\n",
    "\n",
    "  return topic_list\n",
    "\n",
    "# The first part of feature selection\n",
    "# Get a dictionary that has scores for all words.\n",
    "# Words with a nan score will be removed.\n",
    "def get_words_with_high_scores(sentences, labels):\n",
    "    # Data preprocessing - Tokenize and create a word frequency table\n",
    "    word_freq = {}\n",
    "    for sentence, label in zip(sentences, labels):\n",
    "        words = sentence.lower().split()  # Split sentence into words\n",
    "        for word in words:\n",
    "            word_freq[word] = word_freq.get(word, {'count': 0, 'label_count': {}})\n",
    "            word_freq[word]['count'] += 1\n",
    "            word_freq[word]['label_count'][label] = word_freq[word]['label_count'].get(label, 0) + 1\n",
    "\n",
    "    # Calculate Pearson correlation and Chi-Square test for each word\n",
    "    word_scores = {}\n",
    "    for word, info in word_freq.items():\n",
    "        presence = np.zeros(len(sentences))\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            if word in sentence.lower().split():\n",
    "                presence[i] = 1\n",
    "\n",
    "        # Pearson correlation coefficient\n",
    "        # if len(presence) < 2 or len(labels) < 2:\n",
    "        #   # Avoid an error here. Happens when the number of reports under a specific category's topic is lower than 2.\n",
    "        #   pearson_corr = 0\n",
    "        # else:\n",
    "        #   pearson_corr, _ = pearsonr(presence, labels)\n",
    "\n",
    "        # Chi-Square test\n",
    "        contingency_table = [[info['label_count'].get(0, 0), info['label_count'].get(1, 0)],\n",
    "                            [info['count'] - info['label_count'].get(0, 0), len(sentences) - info['label_count'].get(1, 0)]]\n",
    "\n",
    "        # Check if all values in the contingency table are non-negative\n",
    "        if all(val >= 0 for row in contingency_table for val in row):\n",
    "            chi2, _, _, _ = chi2_contingency(contingency_table)\n",
    "        else:\n",
    "            chi2 = 0  # Assign a default value\n",
    "\n",
    "\n",
    "        # Get the score\n",
    "        # score = (pearson_corr + chi2) / 2\n",
    "        score = chi2\n",
    "        word_scores[word] = score\n",
    "\n",
    "    # Filter dictionary to remove non-numeric scores (nan value)\n",
    "    filtered_scores = {k: v for k, v in word_scores.items() if isinstance(v, (int, float)) and not np.isnan(v)}\n",
    "    # Sort words by their scores\n",
    "    sorted_word_scores = {k: v for k, v in sorted(filtered_scores.items(), key=lambda item: abs(item[1]), reverse=True)} # absolute value\n",
    "    return sorted_word_scores\n",
    "\n",
    "\n",
    "# Function to filter words based on scores\n",
    "# It will remove the words that are not in the score dictionary (with nan score)\n",
    "# And then it will keep 50% of the words that have the highest scores\n",
    "# Returns a new sentence with selected words.\n",
    "def filter_words_by_score(sentence, scores_dict):\n",
    "    # Split the sentence into words\n",
    "    words = sentence.split()\n",
    "    # Filter out words not in the scores dictionary\n",
    "    words = [word for word in words if word in scores_dict]\n",
    "    # Get scores for words in the sentence and store them in a list\n",
    "    word_scores_list = [scores_dict.get(word, 0) for word in words]\n",
    "    # Sort words based on their scores\n",
    "    sorted_words = [word for _, word in sorted(zip(word_scores_list, words), reverse=True)[:len(words)//2]]\n",
    "    return ' '.join(sorted_words)\n",
    "\n",
    "\n",
    "# This method will perform topic modelling and feature selection\n",
    "# It returns a new df for training/fine-tuning\n",
    "# df is the pre-processed dataset, dataset is 'android' or 'eclipse'\n",
    "def topic_modelling_and_feature_selection(df, dataset):\n",
    "  final_df_list = []\n",
    "  if dataset.lower() == 'android':\n",
    "    categories = categorize_reports(df, [\"assigned\", \"declined\", \"futurerelease\", \"needsinfo\", \"question\", \"released\", \"reviewed\", \"spam\", \"unassigned\", \"unreproducible\", \"workingasintended\"])\n",
    "  else:\n",
    "    categories = categorize_reports(df, [\"fixed\", \"wontfix\", \"invalid\", \"worksforme\", \"not_eclipse\"])\n",
    "\n",
    "\n",
    "  for category in tqdm(categories, desc=f'Topic Modelling and Feature Selection for {dataset}', unit='category'):\n",
    "    topic_list = topic_modelling(category)\n",
    "    for topic in topic_list:\n",
    "      reports = topic['all_textual_data']\n",
    "      labels = topic['Status'].tolist()\n",
    "      # Initialize LabelEncoder\n",
    "      label_encoder = LabelEncoder()\n",
    "      # Fit and transform labels to binary numbers\n",
    "      binary_labels = label_encoder.fit_transform(labels)\n",
    "      # get score dictionray\n",
    "      score_dictionary = get_words_with_high_scores(reports, binary_labels)\n",
    "      # Filter report based on scores, keep 50% of all words\n",
    "      # only if there are more than two words in the report\n",
    "      for index, row in topic.iterrows():\n",
    "        filtered_words = filter_words_by_score(topic.at[index, 'all_textual_data'], score_dictionary)\n",
    "        if len(filtered_words.split()) > 2:\n",
    "          topic.at[index, 'all_textual_data'] = filtered_words\n",
    "      final_df_list.append(topic)\n",
    "\n",
    "  combined_df = pd.concat(final_df_list, ignore_index=True)\n",
    "  return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ED_PS9OROLc_"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import regex as re  # Using 'regex' module for pattern matching\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Method for changing non-duplicate reports' labels to nondup\n",
    "def label_duplicates(status):\n",
    "    if status.lower() == 'duplicate':\n",
    "        return 'duplicate'\n",
    "    else:\n",
    "        return 'nondup'\n",
    "\n",
    "# Function to check if text is in English\n",
    "def is_english(text):\n",
    "    if isinstance(text, str):  # Check if 'text' is a string\n",
    "        non_english_pattern = re.compile(r'[^\\x00-\\x7F]+')  # Pattern to match non-English characters\n",
    "        return not bool(non_english_pattern.search(text))\n",
    "    return False  # If 'text' is not a string, return False\n",
    "\n",
    "# remove rows that is not english\n",
    "def remove_non_english_rows(df, text_column):\n",
    "\n",
    "  df.dropna()  # Drops rows containing any NaN values\n",
    "\n",
    "  # Filter out rows with non-string values or empty strings in text_column\n",
    "  df = df[df[text_column].apply(lambda x: isinstance(x, str) and x.strip() != '')]\n",
    "\n",
    "  # Apply the language detection function to the specified text column\n",
    "  df['is_english'] = df[text_column].apply(lambda x: is_english(x))\n",
    "\n",
    "  # Filter the DataFrame to keep only rows with English text\n",
    "  df = df[df['is_english']]\n",
    "\n",
    "  # Drop the temporary 'is_english' column\n",
    "  df.drop(columns=['is_english'], inplace=True)\n",
    "\n",
    "\n",
    "  # Reset the index\n",
    "  df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "  return df\n",
    "\n",
    "# Change the column names and the labels\n",
    "def change_column_names_and_labels(df):\n",
    "    # Renaming columns\n",
    "    df.rename(columns={'all_textual_data': 'text', 'Status': 'label'}, inplace=True)\n",
    "\n",
    "    # Mapping values in the 'label' column\n",
    "    label_mapping = {'duplicate': 1, 'nondup': 0}  # Define mapping\n",
    "    df['label'] = df['label'].map(label_mapping).fillna(df['label'])  # Map values and fill NaNs with original values\n",
    "\n",
    "    return df\n",
    "\n",
    "# Oversampling\n",
    "def oversample_textual_data(df, text_column, label_column):\n",
    "\n",
    "  if True:\n",
    "    return df\n",
    "\n",
    "  # Extract text and labels from the DataFrame\n",
    "  X = df[text_column]\n",
    "  y = df[label_column]\n",
    "\n",
    "  # Convert text data to numerical representations (TF-IDF in this example)\n",
    "  vectorizer = TfidfVectorizer()  # Use TF-IDF or other text vectorization methods as needed\n",
    "  X_tfidf = vectorizer.fit_transform(X)\n",
    "\n",
    "  # Apply ADASYN to oversample the minority class\n",
    "  adasyn = ADASYN(random_state=42)\n",
    "  X_resampled, y_resampled = adasyn.fit_resample(X_tfidf, y)\n",
    "\n",
    "  # Convert oversampled data back to a DataFrame\n",
    "  oversampled_df = pd.DataFrame(X_resampled, columns=vectorizer.get_feature_names())\n",
    "  oversampled_df[label_column] = y_resampled\n",
    "\n",
    "  return oversampled_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AKLQLuk8d7CB",
    "outputId": "c4b8b647-766a-4771-905f-2f1ec22956c3"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "  Android dataset (train and test set) ready for use\n",
    "  (to add additional data, drop extra columns, then they can be used\n",
    "   for normal BERT model training)\n",
    "   From Frank: P.S. Already exported two csv files reaady to use. Ask me (Frank) to share them if you beat me to it\n",
    "\"\"\"\n",
    "# train\n",
    "df_train = android_train\n",
    "df_train['all_textual_data'] = df_train['Title'].astype(str) + df_train['Description'].astype(str)\n",
    "df_train['all_textual_data'] = df_train['all_textual_data'].apply(preprocessing_text)\n",
    "processed_df_train_android = topic_modelling_and_feature_selection(df_train, 'android')\n",
    "# Apply the function to update the 'status' column\n",
    "processed_df_train_android['Status'] = processed_df_train_android['Status'].apply(lambda x: label_duplicates(x))\n",
    "# test\n",
    "df_test = android_test\n",
    "df_test['all_textual_data'] = df_test['Title'].astype(str) + df_test['Description'].astype(str)\n",
    "df_test['all_textual_data'] = df_test['all_textual_data'].apply(preprocessing_text)\n",
    "processed_df_test_android = topic_modelling_and_feature_selection(df_test, 'android')\n",
    "# Apply the function to update the 'status' column\n",
    "processed_df_test_android['Status'] = processed_df_test_android['Status'].apply(lambda x: label_duplicates(x))\n",
    "\n",
    "android_train_processed = processed_df_train_android[['all_textual_data', 'Status']].copy()\n",
    "android_test_processed = processed_df_test_android[['all_textual_data', 'Status']].copy()\n",
    "\n",
    "android_train_processed = remove_non_english_rows(android_train_processed, 'all_textual_data')\n",
    "android_test_processed = remove_non_english_rows(android_test_processed, 'all_textual_data')\n",
    "c = change_column_names_and_labels(android_train_processed)\n",
    "android_test_processed = change_column_names_and_labels(android_test_processed)\n",
    "\n",
    "#oversample training set\n",
    "android_train_processed = oversample_textual_data(android_train_processed, 'text', 'label')\n",
    "\n",
    "android_train_processed.to_csv(\"android_train.csv\")\n",
    "android_test_processed.to_csv(\"android_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-3i5OeyEd8nU",
    "outputId": "4372b58d-f4e5-4ce1-df65-7478b1fce754"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "  From Frank: Timed out when running this block. Will try it tomorrow\n",
    "\n",
    "  Eclipse dataset (train and test set) ready for use\n",
    "  (to add additional data, drop extra columns, then they can be used\n",
    "   for normal BERT model training)\n",
    "\"\"\"\n",
    "# Eclipse Dataset\n",
    "# train\n",
    "df_train_eclipse = eclipse_train\n",
    "df_train_eclipse['all_textual_data'] = df_train_eclipse['Title'].astype(str) + df_train_eclipse['Description'].astype(str)\n",
    "df_train_eclipse['all_textual_data'] = df_train_eclipse['all_textual_data'].apply(preprocessing_text)\n",
    "processed_df_train_eclipse = topic_modelling_and_feature_selection(df_train_eclipse, 'eclipse')\n",
    "# Apply the function to update the 'status' column\n",
    "processed_df_train_eclipse['Status'] = processed_df_train_eclipse['Status'].apply(lambda x: label_duplicates(x))\n",
    "eclipse_train = processed_df_train_eclipse[['all_textual_data', 'Status']].copy()\n",
    "\n",
    "eclipse_train = remove_non_english_rows(eclipse_train, 'all_textual_data')\n",
    "eclipse_train = change_column_names_and_labels(eclipse_train)\n",
    "#oversample training set\n",
    "eclipse_train = oversample_textual_data(eclipse_train, 'text', 'label')\n",
    "eclipse_train.to_csv(\"eclipse_train.csv\")\n",
    "# test\n",
    "df_test_eclipse = eclipse_test\n",
    "df_test_eclipse['all_textual_data'] = df_test_eclipse['Title'].astype(str) + df_test_eclipse['Description'].astype(str)\n",
    "df_test_eclipse['all_textual_data'] = df_test_eclipse['all_textual_data'].apply(preprocessing_text)\n",
    "processed_df_test_eclipse = topic_modelling_and_feature_selection(df_test_eclipse, 'eclipse')\n",
    "# Apply the function to update the 'status' column\n",
    "processed_df_test_eclipse['Status'] = processed_df_test_eclipse['Status'].apply(lambda x: label_duplicates(x))\n",
    "eclipse_test = processed_df_test_eclipse[['all_textual_data', 'Status']].copy()\n",
    "\n",
    "eclipse_test = remove_non_english_rows(eclipse_test, 'all_textual_data')\n",
    "eclipse_test = change_column_names_and_labels(eclipse_test)\n",
    "eclipse_test.to_csv(\"eclipse_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "daaki9cISs49"
   },
   "source": [
    "# Adding Additional Data\n",
    "\n",
    "Adding other features into the textual data (including numerical data if possible)\n",
    "\n",
    "And prepare the final dataframes ready to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LPelLAsJSsYI"
   },
   "outputs": [],
   "source": [
    "# TODO: add the code\n",
    "# TODO: If we're adding oversampling, we can do it here.\n",
    "\n",
    "# For Android\n",
    "features_to_add = ['Type']\n",
    "for feature_to_add in features_to_add:\n",
    "  processed_df_train_android['all_textual_data'] = processed_df_train_android['all_textual_data'] + \" \" + processed_df_train_android[feature_to_add].astype(str)\n",
    "  processed_df_test_android['all_textual_data'] = processed_df_test_android['all_textual_data'] + \" \" + processed_df_test_android[feature_to_add].astype(str)\n",
    "\n",
    "\n",
    "# Only get the textual data + labels and save them as the final dataframes to use for training\n",
    "android_train_additional_data = processed_df_train_android[['all_textual_data', 'Status']].copy()\n",
    "android_test_additional_data = processed_df_test_android[['all_textual_data', 'Status']].copy()\n",
    "\n",
    "# Apply the function to update the 'status' column\n",
    "android_train_additional_data['Status'] = android_train_additional_data['Status'].apply(lambda x: label_duplicates(x))\n",
    "android_test_additional_data['Status'] = android_test_additional_data['Status'].apply(lambda x: label_duplicates(x))\n",
    "\n",
    "android_train_additional_data = remove_non_english_rows(android_train_additional_data, 'all_textual_data')\n",
    "android_test_additional_data = remove_non_english_rows(android_test_additional_data, 'all_textual_data')\n",
    "\n",
    "android_train_additional_data = change_column_names_and_labels(android_train_additional_data)\n",
    "android_test_additional_data = change_column_names_and_labels(android_test_additional_data)\n",
    "\n",
    "#oversample training set\n",
    "android_train_additional_data = oversample_textual_data(android_train_additional_data, 'text', 'label')\n",
    "\n",
    "android_train_additional_data.to_csv(\"android_train_additional_data.csv\")\n",
    "android_test_additional_data.to_csv(\"android_test_additional_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hPeRZz4DLHVU"
   },
   "outputs": [],
   "source": [
    "# For Eclipse\n",
    "features_to_add = ['Product', 'Component', 'Type']\n",
    "for feature_to_add in features_to_add:\n",
    "  processed_df_train_eclipse['all_textual_data'] = processed_df_train_eclipse['all_textual_data'] + \" \" + processed_df_train_eclipse[feature_to_add].astype(str)\n",
    "  processed_df_test_eclipse['all_textual_data'] = processed_df_test_eclipse['all_textual_data'] + \" \" + processed_df_test_eclipse[feature_to_add].astype(str)\n",
    "\n",
    "eclipse_train_additional_data = processed_df_train_eclipse[['all_textual_data', 'Status']].copy()\n",
    "eclipse_test_additional_data = processed_df_test_eclipse[['all_textual_data', 'Status']].copy()\n",
    "\n",
    "# Apply the function to update the 'status' column\n",
    "eclipse_train_additional_data['Status'] = eclipse_train_additional_data['Status'].apply(lambda x: label_duplicates(x))\n",
    "eclipse_test_additional_data['Status'] = eclipse_test_additional_data['Status'].apply(lambda x: label_duplicates(x))\n",
    "\n",
    "eclipse_train_additional_data = remove_non_english_rows(eclipse_train_additional_data, 'all_textual_data')\n",
    "eclipse_test_additional_data = remove_non_english_rows(eclipse_test_additional_data, 'all_textual_data')\n",
    "\n",
    "eclipse_train_additional_data = change_column_names_and_labels(eclipse_train_additional_data)\n",
    "eclipse_test_additional_data = change_column_names_and_labels(eclipse_test_additional_data)\n",
    "\n",
    "#oversample training set\n",
    "eclipse_train_additional_data = oversample_textual_data(eclipse_train_additional_data, 'text', 'label')\n",
    "\n",
    "eclipse_train_additional_data.to_csv(\"eclipse_train_additional_data.csv\")\n",
    "eclipse_test_additional_data.to_csv(\"eclipse_test_additional_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}