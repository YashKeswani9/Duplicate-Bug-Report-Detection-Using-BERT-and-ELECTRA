# Duplicate Bug Report Detection â€” NLP Pipeline (Data Prep + Model Training)

Endâ€‘toâ€‘end notebooks for preparing text data and training transformerâ€‘based models for duplicate bug report detection. The workflow covers classic NLP preprocessing, topic modeling & feature selection, class imbalance handling, and fineâ€‘tuning BERT/ELECTRA with evaluation.

## ðŸ“‚ Repository Structure

```
.
â”œâ”€â”€ nlp_project_data_preparation.ipynb   # Data cleaning, preprocessing, topic modeling, feature selection, ADASYN oversampling
â””â”€â”€ nlp_project_model_training.ipynb     # BERT/ELECTRA training (default & tuned), evaluation and reporting
```

## ðŸ§  Whatâ€™s Inside

### 1) `nlp_project_data_preparation.ipynb`
- **Setup & Preparation** (Colabâ€‘friendly; includes Google Drive mounting)
- **Preliminary Preprocessing**: lowercasing, tokenization, stopword removal, stemming/normalization, repeatedâ€‘token cleanup
- **Topic Modeling & Feature Selection**: LDA/Gensim or sklearn LDA to cluster reports by topic; perâ€‘topic feature selection for stronger signals
- **Class Imbalance**: `imblearn` **ADASYN** oversampling to balance positive/negative classes
- **Outputs**: curated DataFrames ready for model training/evaluation

**Key libraries**: `nltk`, `gensim`, `scikit-learn`, `imblearn`, `scipy`, `pandas`, `numpy`

### 2) `nlp_project_model_training.ipynb`
- **Setup & Preparation** (Colabâ€‘friendly; includes Google Drive mounting)
- **Models**:
  - **BERT** â€” baseline (default parameters)
  - **BERT (hyperparameterâ€‘tuned)**
  - **ELECTRA (fineâ€‘tuned)**
- **Evaluation**: accuracy, precision, recall, F1; dataset splits & metrics reporting
- **Artifacts**: trained model weights/checkpoints (if you save them during runs)

**Key libraries**: `transformers`, `torch`, `pandas`, `scikit-learn`

## ðŸš€ Getting Started

> You can run these notebooks either in **Google Colab** or **locally**. Colab is the quickest way to reproduce.

### Option A â€” Google Colab (recommended)
1. Upload this repository to your Google Drive or open the notebooks directly in Colab.
2. Run the **Setup** cell to mount Drive if prompted.
3. Set the input/output paths in the notebook cells (as needed) and run all cells **top to bottom** in each notebook, in order:
   1. `nlp_project_data_preparation.ipynb`
   2. `nlp_project_model_training.ipynb`

### Option B â€” Local environment
1. **Python 3.10+** is recommended.
2. Create and activate a virtual environment:
   ```bash
   python -m venv .venv
   source .venv/bin/activate  # Windows: .venv\Scripts\activate
   ```
3. Install dependencies:
   ```bash
   pip install -U pip
   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu  # or CUDA wheel if using GPU
   pip install transformers scikit-learn pandas numpy scipy nltk gensim imbalanced-learn matplotlib tqdm
   ```
4. Launch Jupyter:
   ```bash
   pip install jupyter
   jupyter notebook
   ```
5. Open and run the notebooks in the same order as above.

## ðŸ—‚ï¸ Data

- Place your raw text/CSV files in a known path (e.g., under `data/` or your Drive path).
- Update any path variables at the top of the notebooks to point to your datasets.
- The **data preparation** notebook produces cleaned feature tables suitable for training (saved via pandas). The **training** notebook expects those outputs.

> **Note**: If youâ€™re using an imbalanced dataset for duplicate vs nonâ€‘duplicate labels, keep ADASYN enabled in the data prep phase.

## âš™ï¸ Configuration Tips

- **Tokenizer & Max Length**: Ensure the tokenizer (BERT/ELECTRA) and `max_length` match across training and evaluation.
- **Batch Size**: Adjust per your GPU/CPU memory. Start small (e.g., 16) and scale up.
- **Learning Rate & Epochs**: Reasonable starting points are 2eâ€‘5 to 5eâ€‘5 for 3â€“5 epochs for BERTâ€‘like models.
- **Reproducibility**: Set random seeds in numpy/torch and log your training/eval parameters.

## ðŸ“Š Metrics & Reporting

The training notebook computes **accuracy**, **precision**, **recall**, and **F1**. Consider persisting a classification report and confusion matrix for quick comparisons across runs.

## ðŸ“ˆ Results (from prior runs)

> These results are **reported from completed experiments** and included here for reference. The shared notebooks are docsâ€‘only and donâ€™t reâ€‘run training by default.

| Dataset | Best Model | Accuracy |
|---|---|---|
| Android | BERT (tuned) | **90.20%** |
| Eclipse | BERT (fineâ€‘tuned) | **83.92%** |

*Note:* Exact bestâ€‘model per dataset may vary by seed and tuning; see your experiment logs/checkpoints if you choose to include them later.

## ðŸ§ª Extending the Work

- Try **perâ€‘topic classifiers** (one model per LDA topic) vs a single global model.
- Add **feature unions**: classic TFâ€‘IDF features sideâ€‘byâ€‘side with transformer embeddings.
- Experiment with **threshold tuning** and **calibration** (e.g., Platt scaling).
- Perform **Kâ€‘fold** crossâ€‘validation for more robust estimates.

## ðŸ’¾ Saving Artifacts

If you save model checkpoints, push them via Git LFS or store externally (e.g., Drive, S3, or a model registry) to keep the repo lightweight.

## âœ… How to Reproduce (Quick Checklist)

- [ ] Run `nlp_project_data_preparation.ipynb` and export the processed training/validation/test frames
- [ ] Run `nlp_project_model_training.ipynb` to fineâ€‘tune **BERT** and **ELECTRA**
- [ ] Capture metrics (accuracy/precision/recall/F1) and optionally persist a CSV/JSON log
- [ ] (Optional) Save best checkpoints & tokenizer to a `/models` folder or remote storage
